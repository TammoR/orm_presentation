#+TITLE: The OrMachine
# #+AUTHOR: Yau Group meeting
#+DATE: March 29, 2017
#+email: Tammo Rukat
#+AUTHOR: Tammo Rukat

# Careful: the ox-reveal.el that is acutally being used is in .emacs.d/elpa/ox-reveal-20150408.831
# #+REVEAL_ROOT: file:./org_reveal_presentation/
#+REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/3.0.0/
# #+REVEAL_EXTRA_CSS: ./local.css
#+OPTIONS: reveal_single_file:t 
#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:nil reveal_control:f
#+OPTIONS: reveal_mathjax:t reveal_rolling_links:f reveal_keyboard:t reveal_overview:t num:nil
#+OPTIONS: reveal_width:1920 reveal_height:1080
#+OPTIONS: toc:1
#+REVEAL_MARGIN: 0.15
#+REVEAL_MIN_SCALE: 0.5
#+REVEAL_MAX_SCALE: 2
#+REVEAL_TRANS: cube 
# default|cube|page|concave|zoom|linear|fade|none.
#+REVEAL_THEME: sky
 # sky, league, moon, solarized, league
#+REVEAL_HLEVEL: 1
#+REVEAL_PLUGINS: (notes highlight markdown)
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_DEFAULT_FRAG_STYLE: roll-in
#+REVEAL_TITLE_SLIDE_BACKGROUND: ./logo.png
#+REVEAL_TITLE_SLIDE_BACKGROUND_SIZE: 400px
#+REVEAL_TITLE_SLIDE_BACKGROUND_REPEAT: repeat
#+REVEAL_TITLE_SLIDE_TEMPLATE: <h1>%t</h1><br><br><br><br><br><h2>Bayesian Boolen matrix factorisation</h2>
#+OPTIONS: org-reveal-center:t

* Introduction to Latent Variable Models 
#+BEGIN_NOTES
- I am sure you are all familiar with the notion of latent variable models. However, I'd like to use this section to introduce the notation and to set the stage for what follows.
- Latent variables are often thought of as underlying, unobserved properties that explain the observed data. For example states of disease that explain physiological parameters. Another example for a latent variable could be a persons particular taste that explains which product they buy or which movie they watch.
#+END_NOTES
** Notation and Graphical Model
#+BEGIN_NOTES
- graphical model (directed = Bayes nets) encodes independence properties
- Everyone familiar with plate notation?
- from a frequentist point of view there is a difference between latent variables and paramters, for a Bayesian there isn't.
- continuous variables are often inappropriate
#+END_NOTES
#+REVEAL_HTML: <div class="column" style="float:left; width: 50%">
#+ATTR_REVEAL: :frag (appear appear appear) :frag_idx(1)
[[./plate_model.png]]
#+ATTR_REVEAL: :frag (appear appear appear) :frag_idx(1)
- Mixture models
- Factor Analysis (PCA)
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div class="column" style="float:left; width: 50%">
#+ATTR_REVEAL: :frag appear
#+ATTR_REVEAL: :frag (appear appear appear appear appear appear) :frag_idx(2 2 2 2 2 2)
- Variables
  + ${x_{nd}}$ -- observations
  + ${u_{ld}}$ -- parameters (globale variables, weights)
  + ${z_{nl}}$ -- latent variables (local variables)
- Indices
  + ${n = 1\ldots N}$ -- observations/specimens
  + ${d = 1\ldots D}$ -- features (e.g. pixels or genes)
  + ${l = 1\ldots L}$ -- latent dimensions
  + ${k = 1\ldots K}$ -- layers
- N observations 
- D features 
- L latent variables
- K layers / abstraction levels
#+REVEAL_HTML: </div>

** Neural network
[[./single_layer_network.png]]
#+ATTR_REVEAL: :frag (appear appear appear) :frag_idx(1)
- Major difference to feed forward neural nets: Nodes *and* weights are stochastic
** What makes a good latent variable model for biological data?
#+BEGIN_NOTES
- Latent properties will often be discrete.
- Scale well
#+END_NOTES
* The OrMachine
$$p(x_{nd}|\mathbf{u},\mathbf{z},\lambda)=
\sigma\left[ \lambda \right]\;\text{if}\;x=\min(1,\mathbf{u}_d^T\mathbf{z}_n)\;\text{else}:\;1-\sigma\left[ \lambda \right] \\
 = \sigma\left[\lambda \tilde{x}_{nd} (1-2\prod\limits_{l}(1-z_{nl}u_{ld}) \right]$$
#+ATTR_REVEAL: :frag appear
#+ATTR_HTML: :width 45% :height 45%
[[./calc_digit_intro.png]]
#+BEGIN_NOTES
- This is a probabilistic model for *Boolean matrix factorisation*.
- Likelihood can be efficiently evaluated
#+END_NOTES
** Dispersion paramter $\lambda$
- Introducing a count variable $P$, that counts how many entries $x_{nd}$ are correctly predicted by the deterministic model $$ P = \sum\limits_{n,d} I\left[x_{nd}=(1-2\prod\limits_{l}(1-z_{nl}u_{ld})\right] $$
- We can rewrite the likelihood
 $$ L = \sigma(\lambda)^P \sigma(-\lambda)^{(ND-P)} $$
- We find the maximum likelihood estimate of $\sigma(\lambda)$ as the fraction of entries that is aligned with the model prediction:
$$ \sigma(\lambda)_{\text{mle}} =\frac{P}{ND}\;. $$
** Multi-layer OrMachine
[[./twolayer_hm.png]]

With ${\mathbf{z}^{[0]}_n = \mathbf{x}_n}$ and ${L^{[0]} = D}$, that is
$$  p(\mathbf{Z}^{[0:K]},\mathbf{U}^{[1:K]},\lambda) = 
  p(\mathbf{Z}^{[K]}) \prod_{k=0}^{K-1} p(\mathbf{Z}^{[k]}|\mathbf{Z}^{[k{+}1]},\mathbf{U}^{[k{+}1]},\lambda^{[k{+}1]})\, p(\mathbf{U}^{[k{+}1]})\, p(\lambda^{[k{+}1]}) 
$$

The joint density factorises in terms of the form p(layer|parents)

* Inference for the OrMachine
** Full conditionals
#+ATTR_REVEAL: :frag (appear appear roll-in) :frag_idx (1 2 3)
- Likelihood $$L = \sigma\left[\lambda \tilde{x}_{nd} (1-2\prod\limits_{l}(1-z_{nl}u_{ld}) \right]$$
- Conditional for $z_{nl}$ $$ p(z_{nl}|\text{rest}) = \sigma\left[\lambda \tilde{z}_{nl} \sum\limits_d \tilde{x}_{nd}\; u_{ld}\prod\limits_{l^*\neq l} (1-z_{nl^*}u_{l^*d})\right] $$
- Fast computation: One criterium fullfilled $\rightarrow$ the value of the term inside the sum over $d$ is known without considering its remaining terms.
  - $u_{ld} = 0$. The effect of $z_{nl}$ on the likelihood  absent if $u_{ld}=0$.
  - $z_{nl^*}u_{l^*d} = 1$ for $l^* \neq l$. Another parent of $x_{nd}$ is already 1. Any additional parents will have no effect.
** Implementation
#+ATTR_HTML: :width 40% :height 50%
[[./alg1_2.png]]

# *** A little detour: Peskun's Theorem
# #+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (1 4)
# - We have
#   #+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (1 2 3)
#   - A random variable $X$ following a distribution $\pi$ 
#   - Transition matrices $P_1$ and $P_2$ that are reversible for $\pi$: $$ \pi(x)P(x,y) = \pi(y)P(y,x) $$
#   - Define $P_2 \ge P_1$, if it's true for every off-diagonal element.
# - The theorem states, if $$P_2 \ge P_1$$ then:
#   $$ v(f, \pi, P_1) \ge v(f, \pi, P_2) $$ where $$ v(f, \pi, P) = \lim_{N\rightarrow\infty} N \text{var}(\hat{I}_N) $$ is the variance of some estimator $$ \hat{I}_N = \sum\limits_{t=1}^N \frac{f(X^{(t)})}{N}\;\; \text{of}\;\; I = E_{\pi}(f)$$
** A modified binary state Gibbs sampler
#+ATTR_REVEAL: :frag (appear appear appear appear) :frag_idx (1 2 4 6)
- Discrete variables. New state $\mathbf{y}$, old state $\mathbf{x}$.
- In a Gibbs sampler we draw a new value $y_i$ from $$ \pi(y_i|x_{-i}) $$
  #+ATTR_REVEAL: :frag (appear) :frag_idx (3)
  - The Metropolis Hasting acceptance probability is always 1. $$ $$
- In the modified sampler we draw a new  $y_i$, *different from the old value* $x_i$ with probability $$ 1 = \frac{\pi(y_i|x_{-i})}{1-\pi(x_i|x_{-i})}\;. $$
  #+ATTR_REVEAL: :frag (appear) :frag_idx (5)
  - Acceptance probability = mutation probability: $$\alpha=\frac{\pi(y_i|x_{-i})}{\pi(x_i|x_{-i})(y_i,x_i)} = \frac{\pi(y_i|x_{-i})}{1-\pi(y_i|x_{-i})} $$
- Off diagonal elements of the transition matrix:
  $$  \frac{\pi(y_i|x_{-i})}{1-\pi(y_i|x_{-i})} \ge \pi(y_i|x_{-i}) $$
** Metropolised Gibbs sampler - Algorithm
#+ATTR_HTML: :width 40% :height 50%
[[./alg2.png]]
** Speed
[[./scaling_parallel.png]]
* Examples and Experiments
** Random matrix factorisation
*** Problem setting
#+ATTR_HTML: :width 50% :height 50%
[[./factorisation.png]]
*** Performance
#+ATTR_HTML: :width 35% :height %0%
[[./factorsiation_performance_new.png]]
** Random matrix completion
- Remember likelihood and conditionals: $$L = \sigma\left[\lambda \tilde{x}_{nd} (1-2\prod\limits_{l}(1-z_{nl}u_{ld}) \right]$$ $$ p(z_{nl}|\text{rest}) = \sigma\left[\lambda \tilde{z}_{nl} \sum\limits_d \tilde{x}_{nd}\; u_{ld}\prod\limits_{l^*\neq l} (1-z_{nl^*}u_{l^*d})\right] $$
- Set an unobserved observation to $x_{nd} = 0.5 \;\rightarrow\; \tilde{x}_{nd}=0$
#+REVEAL_HTML: <div class="column" style="float:left; width: 50%">
#+ATTR_HTML: :width 80% :height 50%
[[./completion1.png]]
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div class="column" style="float:left; width: 50%">
#+ATTR_HTML: :width 85% :height 50%
[[./completion2.png]]
#+REVEAL_HTML: </div>
** MovieLense
#+REVEAL_HTML: <div class="column" style="float:left; width: 50%">
#+ATTR_HTML: :width 85% :height 50%
[[./movielense1.png]]
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div class="column" style="float:left; width: 50%">
#+ATTR_HTML: :width 85% :height 50%
[[./ml_roc.png]]
#+REVEAL_HTML: </div>
** Calculator digits
[[./calc_hierarchy.png]]
*** MNIST
#+ATTR_HTML: :width 60% :height 50%
[[./mnist_hierarchy.png]]
#+BEGIN_NOTES
- Explain overfitting (get more and more distributed representation)
#+END_NOTES
** Single cell data I
[[./sc_hierarchy.png]]
** Single cell data II - 1.3 Million Brain Cells from E18 Mice
*** Gene patterns
#+ATTR_HTML: :width 90% :height 50%
[[./mice_neurons1.png]]
*** Specimen representations
#+ATTR_HTML: :width 90% :height 50%
[[./mice_neurons2.png]]
** Deep calculator digits
[[./calc_digit_intro.png]]
#+ATTR_HTML: :width 50% :height 50%
[[./deep_calc.png]]
- Second layer representation fed forward to data layer.
** Deep noisy calculator digits
#+ATTR_HTML: :width 50% :height 50%
[[./deeper_calc.png]]
- Input: 50 digits with 70% missing observations
- Reducde reconstruction error from 1.4% to 0.4% compared to shallow model
** Pancan and pathway data
*** Setup
[[./arc.png]]
*** "Data"
# [[./arc_data_2.png]]
#+ATTR_HTML: :width 55% :height 30%
[[./fa_data_prob.png]]
*** Embedding
# [[./arc_patterns_2.png]]
#+REVEAL_HTML: <div class="column" style="float:left; width: 50%">
#+ATTR_HTML: :width 95% :height 100%
[[./fa_pw_latent.png]]
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div class="column" style="float:left; width: 50%">
#+ATTR_HTML: :width 95% :height 100%
[[./fa_pw_patterns.png]]
#+REVEAL_HTML: </div>
*** Predictive
# [[./arc_codes_2.png]]
#+ATTR_HTML: :width 52% :height 62%
[[./fa_pw_predictive.png]]
* Additional Material
** Preprint on ArXiv
#+ATTR_HTML: :width 90% :height 90%
[[./arxiv.png]]
** Hamming Machine
#+ATTR_REVEAL: :frag (appear appear appear appear) :frag_idx (1 2 3 4)
- Construct a probability distribution based on the hamming distance between two binary vectors, ${h(\mathbf{x},\mathbf{u})}$, and a dispersion parameter ${\lambda}$: $$ p(\mathbf{x}|\mathbf{u}) \propto \exp\left[ -\lambda \, h(\mathbf{x},\mathbf{u}) \right] $$
- Each observations ${\mathbf{x} }$ is generated from a subset of binary *codes*: ${\mathbf{u}_{l{=}1\ldots L}}$, selected by a vector of binary latent variables ${\mathbf{z}}$ $$ p(\mathbf{x}|\mathbf{U},\mathbf{z},\lambda) \propto \prod\limits_l p(\mathbf{x}|\mathbf{u}_l,\lambda)^{z_l} = \prod\limits_d \exp\left[- \sum_l z_l \lambda h(x_d,u_{ld}) \right]$$
- Normalising the likelihood for for binary observations yields a *logistic sigmoid*: $$ p(x_d = 1|\mathbf{z}, \mathbf{u}_{1\ldots L}, \lambda) = \frac{1}{1+\exp\left[-\lambda \sum\limits_l z_l (2u_{ld} - 1) \right]} = \sigma\left[\lambda \sum_l z_l \tilde{u}_{ld} \right]$$
- We defined the mapping from ${\{0,1\}}$ to ${\{{-}1,1\}\,}$: $\;\;{\tilde{u} = 2u{-}1}$ 
#+BEGIN_NOTES
  - We use the tilde mapping throughout
  - This migh be a bit unconventional
#+END_NOTES
** One-hot sampling

