<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>The OrMachine</title>
<meta name="author" content="(Tammo Rukat)"/>

<link rel="stylesheet" href="./reveal.js/css/reveal.css"/>
<link rel="stylesheet" href="./reveal.js/css/theme/sky.css" id="theme"/>

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = './reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide" data-background="./logo.png" data-background-size="400px" data-background-repeat="repeat">
<h1>The OrMachine</h1><br><br><br><br><br><h2>Bayesian Boolen matrix factorisation</h2>
</section>

<section>
<section id="slide-sec-1">
<h2 id="sec-1">Boolean Matrix Factorisation</h2>

<div class="figure">
<p><img src="./calc_digit_data.png" alt="calc_digit_data.png" class="fragment appear" width="40%" height="40%" />
</p>
</div>

<div class="figure">
<p><img src="./calc_digit_factor.png" alt="calc_digit_factor.png" class="fragment appear" width="40%" height="40%" />
</p>
</div>
<span class="fragment appear"><p>
Example
</p></span>

<div class="figure">
<p><img src="./calc_example.png" alt="calc_example.png" class="fragment appear" width="45%" height="45%" />
</p>
</div>
</section>
<section id="slide-sec-1-1">
<h3 id="sec-1-1">Probabilistic Generative Model</h3>
<div class="column" style="float:left; width: 45%">

<div class="figure">
<p><img src="./calc_digit_intro.png" alt="calc_digit_intro.png" width="90%" height="90%" />
</p>
</div>
</div>

<div class="column" style="float:right; width: 55%">
<span class="fragment (appear appear) :frag_idx(1)"><p>
Notation
</p></span>
<ul class="fragment appear">
<li>\({x_{nd}}\) &#x2013; observations</li>
<li>\({u_{ld}}\) &#x2013; codes (globale variables)</li>
<li>\({z_{nl}}\) &#x2013; latent variables (local variables)</li>
<li>\(\lambda\) &#x2013; global noise parameter</li>

</ul>

<span class="fragment (appear) :frag_idx(2)"><p>
Definitions
</p></span>
<ul class="fragment appear">
<li>Logistic sigmoid: \(\sigma(x) = (1+\exp[-x])^{-1}\)</li>
<li>Mapping \(\{0,1\}\) to \(\{-1,1\}\): \(\tilde{x} = 2x-1\)</li>

</ul>
</div>

<div class="column" style="float:right; width: 100%">
<span class="fragment appear"><p>
$$  p(x_{nd}|\mathbf{u},\mathbf{z},\lambda)= \begin{cases} \sigma [ \lambda];\;&\text{if}\;x=\min(1,\mathbf{z}_n^T\mathbf{u}_d)\;\; \\ 1-\sigma [ \lambda]=\sigma[-\lambda];\;&\text{if}\;x\neq\min(1,\mathbf{z}_n^T\mathbf{u}_d)    \end{cases}
$$
</p></span>
<span class="fragment appear"><p>
$$\;\;\;\; = \sigma\left[\lambda \tilde{x}_{nd} \left(1-2\prod\limits_{l}(1-z_{nl}u_{ld})\right) \right]$$
</p></span>
</div>
<aside class="notes">
<ul class="org-ul">
<li>This is a probabilistic model for <b>Boolean matrix factorisation</b>.
</li>
<li>Likelihood can be efficiently evaluated
</li>
<li>Example topic models
</li>
</ul>

</aside>
</section>
</section>
<section>
<section id="slide-sec-2">
<h2 id="sec-2">Inference for the OrMachine</h2>
<div class="outline-text-2" id="text-2">
</div></section>
<section id="slide-sec-2-1">
<h3 id="sec-2-1">Full conditionals</h3>
<div class="column" style="float:left; width: 50%">
<ul>
<li data-fragment-index="1" class="fragment appear">Likelihood: $$L = \sigma\bigg[\lambda \tilde{x}_{nd} (1-2\prod\limits_{l}(1-z_{nl}u_{ld}) \bigg]$$</li>
<li data-fragment-index="2" class="fragment appear">Full Conditional: $$ p(z_{nl}|\text{rest}) = \sigma\bigg[\lambda \tilde{z}_{nl} \sum\limits_d \tilde{x}_{nd}\; u_{ld}\prod\limits_{l'\neq l} (1-z_{nl'}u_{l'd})\bigg] $$
<ul data-fragment-index="3" class="fragment appear">
<li>Intuition: Need to consider the full Markov Blanket including <i>explaining away</i> dependencies.</li>

</ul></li>

</ul>
<p>
$$ $$
</p>
<ul>
<li data-fragment-index="4" class="fragment appear"><b>Computational shortcut:</b> Two criteria to skip the remaining inner products
<ul>
<li data-fragment-index="5" class="fragment appear">\(u_{ld} = 0\) \(\rightarrow\) No effect of \(z_{nl}\) on the likelihood</li>
<li data-fragment-index="6" class="fragment appear">\(z_{nl'}u_{l'd} = 1\) for \(l' \neq l\) \(\rightarrow\) \(x_{nd}\) is <i>explained away</i>.</li>

</ul></li>

</ul>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./single_layer_network.png" alt="single_layer_network.png" data-fragment-index="3" class="fragment appear" />
</p>
</div>
</div>
</section>
<section id="slide-sec-2-2">
<h3 id="sec-2-2">Implementation</h3>

<div class="figure">
<p><img src="./alg1_2.png" alt="alg1_2.png" width="50%" height="60%" />
</p>
</div>

</section>
<section id="slide-sec-2-3">
<h3 id="sec-2-3">Dispersion paramter \(\lambda\)</h3>
<ul>
<li>\(P\) counts how many entries \(x_{nd}\) are correctly predicted by the deterministic Boolean matrix product $$ P = \sum\limits_{n,d} I\left[x_{nd}=(1-2\prod\limits_{l}(1-z_{nl}u_{ld})\right] $$</li>
<li>We can rewrite the likelihood
$$ L = \sigma(\lambda)^P \sigma(-\lambda)^{(ND-P)} $$</li>
<li>We find the MLE of \(\sigma(\lambda)\) in <b>closed form</b>:</li>

</ul>
<p>
$$ \sigma(\lambda)_{\text{mle}} =\frac{P}{ND}\;. $$
</p>
</section>
<section id="slide-sec-2-4">
<h3 id="sec-2-4">Metropolised Gibbs sampler - Algorithm</h3>

<div class="figure">
<p><img src="./alg2_mod.png" alt="alg2_mod.png" width="50%" height="60%" />
</p>
</div>
</section>
<section id="slide-sec-2-5">
<h3 id="sec-2-5">A modified binary state Gibbs sampler</h3>
<ul>
<li data-fragment-index="1" class="fragment appear">Old state \(\mathbf{x}\), new state \(\mathbf{y}\).</li>
<li data-fragment-index="2" class="fragment appear">Gibbs sampler: Draw a new value from the full conditional \(p(y|\text{rest})\), acceptance probability 1.</li>
<li data-fragment-index="3" class="fragment appear">Here, we draw \(y\) <b>different from the</b> \(x\) with probability 1.</li>
<li data-fragment-index="4" class="fragment appear">Metropolis-Hasting acceptance probability: $$ \frac{p(y|\text{rest}) q(x|y)}{p(x|\text{rest}) q(y|x)} = \frac{p(y|\text{rest})}{1-p(y|\text{rest})} \ge p(y|\text{rest})$$</li>

</ul>

<ul class="fragment appear">
<li>Typical Gibbs sampler:

<p>
<img src="./heads_small.png" alt="heads_small.png" />   <img src="./heads_small.png" alt="heads_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" /> 
 <img src="./tails_small.png" alt="tails_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./heads_small.png" alt="heads_small.png" /> <img src="./tails_small.png" alt="tails_small.png" /> 
</p></li>

</ul>

<ul class="fragment appear">
<li>Metropolised Gibbs sampler:

<p>
<img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />   <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />
</p></li>

</ul>
</section>
</section>
<section>
<section id="slide-sec-3">
<h2 id="sec-3">Examples and Experiments</h2>
<div class="outline-text-2" id="text-3">
</div></section>
<section id="slide-sec-3-1">
<h3 id="sec-3-1">Random matrix factorisation</h3>
<div class="column" style="float:left; width: 50%">
<p>
$$ $$
<img src="./mp.png" alt="mp.png" />
</p>
<ul>
<li>A similar probabilistic model.</li>
<li>MAP inference using message passing.</li>
<li>Outperforms all previous state-of-the-art methods.</li>

</ul>
<ul data-fragment-index="1" class="fragment appear">
<li><b>OrMachine features consistently lower reconstruction error</b></li>

</ul>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./factorsiation_performance_new.png" alt="factorsiation_performance_new.png" data-fragment-index="1" class="fragment appear" width="70%" height="%0%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-3-2">
<h3 id="sec-3-2">Random matrix completion</h3>
<ul>
<li data-fragment-index="1" class="fragment appear">Handling missing data is easy.Set an unobserved data-point \(x_{nd} = 0.5 \;\rightarrow\; \tilde{x}_{nd}=0\)</li>

</ul>
<span class="fragment (appear)" data-fragment-index="2"><p>
<p>
$$L = \prod\limits_{nd} \sigma\left[\lambda \tilde{x}_{nd} (1-2\prod\limits_{l}(1-z_{nl}u_{ld}) \right]\;\;\;\rightarrow\text{Contribute constant factor}\;\sigma(0)=\frac{1}{2}$$ 
</p>
<span class="fragment (appear)" data-fragment-index="3"><p>
<p>
$$ p(z_{nl}|\text{rest}) = \sigma\left[\lambda \tilde{z}_{nl} \sum\limits_d \tilde{x}_{nd}\; u_{ld}\prod\limits_{l'\neq l} (1-z_{nl'}u_{l'd})\right]\;\;\;\rightarrow \text{No contribution} $$
</p>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./completion1.png" alt="completion1.png" data-fragment-index="4" class="fragment appear" width="80%" height="50%" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./completion2.png" alt="completion2.png" data-fragment-index="5" class="fragment appear" width="85%" height="50%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-3-3">
<h3 id="sec-3-3">MovieLense</h3>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./movielense1.png" alt="movielense1.png" width="85%" height="50%" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./ml_roc.png" alt="ml_roc.png" width="85%" height="50%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-3-4">
<h3 id="sec-3-4">Single cell data II - 1.3 Million Brain Cells x 20k genes (E18 Mice)</h3>
<div class="outline-text-3" id="text-3-4">
</div></section>
<section id="slide-sec-3-4-1">
<h4 id="sec-3-4-1">Calculator digits</h4>

<div class="figure">
<p><img src="./calc_hierarchy_feb21.png" alt="calc_hierarchy_feb21.png" />
</p>
</div>
<ul>
<li>Infer OrMachines of different dimensionality on noise-free calculator digits</li>

</ul>
</section>
<section id="slide-sec-3-4-2">
<h4 id="sec-3-4-2">Gene patterns</h4>

<div class="figure">
<p><img src="./mice_neurons1.png" alt="mice_neurons1.png" width="90%" height="50%" />
</p>
</div>
</section>
<section id="slide-sec-3-4-3">
<h4 id="sec-3-4-3">Specimen representations</h4>

<div class="figure">
<p><img src="./mice_neurons2.png" alt="mice_neurons2.png" width="90%" height="50%" />
</p>
</div>
</section>
<section id="slide-sec-3-5">
<h3 id="sec-3-5">Deep noisy calculator digits</h3>

<div class="figure">
<p><img src="./deeper_calc.png" alt="deeper_calc.png" width="50%" height="50%" />
</p>
</div>
<ul>
<li>Input: 50 digits with 70% missing observations</li>
<li>Reducde reconstruction error from 1.4% to 0.4% compared to shallow model</li>

</ul>
</section>
<section id="slide-sec-3-6">
<h3 id="sec-3-6">Pancan and pathway data</h3>
<div class="outline-text-3" id="text-3-6">
</div></section>
<section id="slide-sec-3-6-1">
<h4 id="sec-3-6-1">Setup: Combine Layers of OrMachines</h4>

<div class="figure">
<p><img src="./arc.png" alt="arc.png" />
</p>
</div>
</section>
<section id="slide-sec-3-6-2">
<h4 id="sec-3-6-2">"Data"</h4>

<div class="figure">
<p><img src="./pancan_data.png" alt="pancan_data.png" width="70%" height="70%" />
</p>
</div>
</section>
<section id="slide-sec-3-6-3">
<h4 id="sec-3-6-3">Embeddinga</h4>

<div class="figure">
<p><img src="./pancan_representations.png" alt="pancan_representations.png" width="80%" height="80%" />
</p>
</div>
</section>
<section id="slide-sec-3-6-4">
<h4 id="sec-3-6-4">Clustering via one-hot activations</h4>

<div class="figure">
<p><img src="./pancan_clustering.png" alt="pancan_clustering.png" width="60%" height="60%" />
</p>
</div>
</section>
</section>
<section>
<section id="slide-sec-4">
<h2 id="sec-4">Conclusion</h2>
<ul>
<li>Boolean Matrix Factorisation is a simple and intuitive model for binary observations.</li>
<li>The OrMachine is a probabilistic model with highly scalable Bayesian inference.</li>
<li>Missing data and prior knowledge can easily be dealt with.</li>
<li>Multi-layer hierarchies can extract remaining correlations and provide a novel way to include additional information</li>

</ul>
</section>
</section>
<section>
<section id="slide-sec-5">
<h2 id="sec-5">Additional Material</h2>
<div class="outline-text-2" id="text-5">
</div></section>
<section id="slide-sec-5-1">
<h3 id="sec-5-1">Auto-Regulating Sparsity</h3>

<div class="figure">
<p><img src="./single_cell_overfit.png" alt="single_cell_overfit.png" width="70%" height="70%" />
</p>
</div>
</section>
<section id="slide-sec-5-2">
<h3 id="sec-5-2">Preprint on ArXiv</h3>

<div class="figure">
<p><img src="./arxiv.png" alt="arxiv.png" width="90%" height="90%" />
</p>
</div>
</section>
<section id="slide-sec-5-3">
<h3 id="sec-5-3">Hamming Machine</h3>
<ul>
<li data-fragment-index="1" class="fragment appear">Construct a probability distribution based on the hamming distance between two binary vectors, \({h(\mathbf{x},\mathbf{u})}\), and a dispersion parameter \({\lambda}\): $$ p(\mathbf{x}|\mathbf{u}) \propto \exp\left[ -\lambda \, h(\mathbf{x},\mathbf{u}) \right] $$</li>
<li data-fragment-index="2" class="fragment appear">Each observations \({\mathbf{x} }\) is generated from a subset of binary <b>codes</b>: \({\mathbf{u}_{l{=}1\ldots L}}\), selected by a vector of binary latent variables \({\mathbf{z}}\) $$ p(\mathbf{x}|\mathbf{U},\mathbf{z},\lambda) \propto \prod\limits_l p(\mathbf{x}|\mathbf{u}_l,\lambda)^{z_l} = \prod\limits_d \exp\left[- \sum_l z_l \lambda h(x_d,u_{ld}) \right]$$</li>
<li data-fragment-index="3" class="fragment appear">Normalising the likelihood for for binary observations yields a <b>logistic sigmoid</b>: $$ p(x_d = 1|\mathbf{z}, \mathbf{u}_{1\ldots L}, \lambda) = \frac{1}{1+\exp\left[-\lambda \sum\limits_l z_l (2u_{ld} - 1) \right]} = \sigma\left[\lambda \sum_l z_l \tilde{u}_{ld} \right]$$</li>
<li data-fragment-index="4" class="fragment appear">We defined the mapping from \({\{0,1\}}\) to \({\{{-}1,1\}\,}\): \(\;\;{\tilde{u} = 2u{-}1}\)</li>

</ul>
<aside class="notes">
<ul class="org-ul">
<li>We use the tilde mapping throughout
</li>
<li>This migh be a bit unconventional
</li>
</ul>

</aside>
</section>
<section id="slide-sec-5-4">
<h3 id="sec-5-4">One-hot sampling</h3>
</section>
<section id="slide-sec-5-5">
<h3 id="sec-5-5">Introduction to Latent Variable Models</h3>
<aside class="notes">
<ul class="org-ul">
<li>I am sure you are all familiar with the notion of latent variable models. However, I'd like to use this section to introduce the notation and to set the stage for what follows.
</li>
<li>Latent variables are often thought of as underlying, unobserved properties that explain the observed data. For example states of disease that explain physiological parameters. Another example for a latent variable could be a persons particular taste that explains which product they buy or which movie they watch.
</li>
</ul>

</aside>
</section>
<section id="slide-sec-5-5-1">
<h4 id="sec-5-5-1">Notation and Graphical Model</h4>
<aside class="notes">
<ul class="org-ul">
<li>graphical model (directed = Bayes nets) encodes independence properties
</li>
<li>Everyone familiar with plate notation?
</li>
<li>from a frequentist point of view there is a difference between latent variables and paramters, for a Bayesian there isn't.
</li>
<li>continuous variables are often inappropriate
</li>
</ul>

</aside>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./plate_model.png" alt="plate_model.png" class="fragment (appear appear appear) :frag_idx(1)" />
</p>
</div>
<ul>
<li class="fragment appear">Mixture models</li>
<li class="fragment appear">Factor Analysis (PCA)</li>

</ul>
</div>
<div class="column" style="float:left; width: 50%">
<ul>
<li class="fragment appear">Variables
<ul>
<li>\({x_{nd}}\) &#x2013; observations</li>
<li>\({u_{ld}}\) &#x2013; parameters (globale variables, weights)</li>
<li>\({z_{nl}}\) &#x2013; latent variables (local variables)</li>

</ul></li>
<li class="fragment appear">Indices
<ul>
<li>\({n = 1\ldots N}\) &#x2013; observations/specimens</li>
<li>\({d = 1\ldots D}\) &#x2013; features (e.g. pixels or genes)</li>
<li>\({l = 1\ldots L}\) &#x2013; latent dimensions</li>
<li>\({k = 1\ldots K}\) &#x2013; layers</li>

</ul></li>
<li class="fragment appear">N observations</li>
<li class="fragment appear">D features</li>
<li class="fragment appear">L latent variables</li>
<li class="fragment appear">K layers / abstraction levels</li>

</ul>
</div>

</section>
<section id="slide-sec-5-5-2">
<h4 id="sec-5-5-2">Neural network</h4>

<div class="figure">
<p><img src="./single_layer_network.png" alt="single_layer_network.png" />
</p>
</div>
<ul>
<li class="fragment appear">Major difference to feed forward neural nets: Nodes <b>and</b> weights are stochastic</li>

</ul>
</section>
<section id="slide-sec-5-5-3">
<h4 id="sec-5-5-3">What makes a good latent variable model for biological data?</h4>
<aside class="notes">
<ul class="org-ul">
<li>Latent properties will often be discrete.
</li>
<li>Scale well
</li>
</ul>

</aside>
</section>
<section id="slide-sec-5-6">
<h3 id="sec-5-6">Multi-layer OrMachine</h3>

<div class="figure">
<p><img src="./twolayer_hm.png" alt="twolayer_hm.png" />
</p>
</div>

<p>
With \({\mathbf{z}^{[0]}_n = \mathbf{x}_n}\) and \({L^{[0]} = D}\), that is
$$  p(\mathbf{Z}^{[0:K]},\mathbf{U}^{[1:K]},\lambda) = 
  p(\mathbf{Z}^{[K]}) \prod_{k=0}^{K-1} p(\mathbf{Z}^{[k]}|\mathbf{Z}^{[k{+}1]},\mathbf{U}^{[k{+}1]},\lambda^{[k{+}1]})\, p(\mathbf{U}^{[k{+}1]})\, p(\lambda^{[k{+}1]}) 
$$
</p>

<p>
The joint density factorises in terms of the form p(layer|parents)
</p>
</section>
<section id="slide-sec-5-7">
<h3 id="sec-5-7">Random matrix factorisation</h3>
<div class="outline-text-3" id="text-5-7">
</div></section>
<section id="slide-sec-5-7-1">
<h4 id="sec-5-7-1">Problem setting</h4>

<div class="figure">
<p><img src="./factorisation.png" alt="factorisation.png" width="50%" height="50%" />
</p>
</div>
</section>
<section id="slide-sec-5-8">
<h3 id="sec-5-8">Speed</h3>

<div class="figure">
<p><img src="./scaling_parallel.png" alt="scaling_parallel.png" />
</p>
</div>
</section>
<section id="slide-sec-5-9">
<h3 id="sec-5-9">Single cell data I</h3>

<div class="figure">
<p><img src="./sc_hierarchy.png" alt="sc_hierarchy.png" />
</p>
</div>
</section>
<section id="slide-sec-5-10">
<h3 id="sec-5-10">MNIST</h3>

<div class="figure">
<p><img src="./mnist_hierarchy.png" alt="mnist_hierarchy.png" width="60%" height="50%" />
</p>
</div>
<aside class="notes">
<ul class="org-ul">
<li>Explain overfitting (get more and more distributed representation)
</li>
</ul>

</aside>
</section>
<section id="slide-sec-5-11">
<h3 id="sec-5-11">Deep calculator digits</h3>

<div class="figure">
<p><img src="./calc_digit_intro.png" alt="calc_digit_intro.png" />
</p>
</div>

<div class="figure">
<p><img src="./deep_calc.png" alt="deep_calc.png" width="50%" height="50%" />
</p>
</div>
<ul>
<li>Second layer representation fed forward to data layer.</li>

</ul>

</section>
<section id="slide-sec-5-12">
<h3 id="sec-5-12">A little detour: Peskun's Theorem</h3>
<ul>
<li data-fragment-index="1" class="fragment appear">We have
<ul>
<li data-fragment-index="1" class="fragment appear">A random variable \(X\) following a distribution \(\pi\)</li>
<li data-fragment-index="2" class="fragment appear">Transition matrices \(P_1\) and \(P_2\) that are reversible for \(\pi\): $$ \pi(x)P(x,y) = \pi(y)P(y,x) $$</li>
<li data-fragment-index="3" class="fragment appear">Define \(P_2 \ge P_1\), if it's true for every off-diagonal element.</li>

</ul></li>
<li data-fragment-index="4" class="fragment appear">The theorem states, if $$P_2 \ge P_1$$ then:
$$ v(f, \pi, P_1) \ge v(f, \pi, P_2) $$ where $$ v(f, \pi, P) = \lim_{N\rightarrow\infty} N \text{var}(\hat{I}_N) $$ is the variance of some estimator $$ \hat{I}_N = \sum\limits_{t=1}^N \frac{f(X^{(t)})}{N}\;\; \text{of}\;\; I = E_{\pi}(f)$$</li>

</ul>
</section>
</section>
</div>
</div>

<script src="./reveal.js/lib/js/head.min.js"></script>
<script src="./reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: true,
rollingLinks: true,
keyboard: true,
overview: true,
width: 1920,
height: 1080,
margin: 0.15,
minScale: 0.50,
maxScale: 2.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'cube', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: './reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: './reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: './reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: './reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } }
]
});
</script>
</body>
</html>
