<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>The OrMachine</title>
<meta name="author" content="(Tammo Rukat)"/>

<link rel="stylesheet" href="./reveal.js/css/reveal.css"/>
<link rel="stylesheet" href="./reveal.js/css/theme/sky.css" id="theme"/>

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = './reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,mathjax_local"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide" data-background="./logo.png" data-background-size="400px" data-background-repeat="repeat">
<h1>The OrMachine</h1><br><br><br><br><br><h2>Bayesian Boolen matrix factorisation</h2>
</section>


<section>
<section id="slide-sec-1">
<h2 id="sec-1">Single Cell Gene Expression</h2>
<div class="column" style="float:left; width: 100%">

<div class="figure">
<p><img src="./mouse_data.png" alt="mouse_data.png" width="80%" height="80%" />
</p>
</div>
</div>
<aside class="notes">
<p>
Requirements
</p>
<ul class="org-ul">
<li>interpretable: intuition for latent variables, quantification of uncertainty
</li>
<li>correspondencen to physical mechanism: discrete latent properties like presence/absence of disease/mutation
</li>
<li>prior knowledge
</li>
<li>scalability
</li>
</ul>

</aside>

</section>
</section>
<section>
<section id="slide-sec-2">
<h2 id="sec-2">Boolean Matrix Factorisation</h2>
<div class="column" style="float:left; width: 50%">
<p>
$$ $$
</p>
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
Observed Data
</p>

<div class="figure">
<p><img src="./calc_digit_data.png" alt="calc_digit_data.png" data-fragment-index="1" class="fragment appear" width="90%" height="90%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 50%">
<span class="fragment (appear)" data-fragment-index="2"><p>
<p>
Factorisation
</p>

<div class="figure">
<p><img src="./calc_digit_factor.png" alt="calc_digit_factor.png" data-fragment-index="2" class="fragment appear" width="90%" height="90%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 100%">
<span class="fragment (appear)" data-fragment-index="3"><p>
<p>
Example
</p>

<div class="figure">
<p><img src="./calc_example.png" alt="calc_example.png" data-fragment-index="3" class="fragment appear)" width="45%" height="45%" />
</p>
</div>
</div>

<aside class="notes">
<ul class="org-ul">
<li>What is BooMF? The approximation of a binary data matrix as product of two low rank binary matrices.
</li>
<li>Like a regular matrix product but thresholded.
</li>
<li>Essentially the model learns the seperate bars from which every digit can be composed. We call them <i>codes</i>
</li>
<li>The L-dimensional indicator Z provides the compact representation of which codes are allocated to each observation.
</li>
<li>Similarity to noisy-OR
</li>
</ul>

</aside>

</section>
<section id="slide-sec-2-1">
<h3 id="sec-2-1">Probabilistic Generative Model</h3>
<div class="column" style="float:right; width: 100%">
<span class="fragment appear"><p>
$$  p(\underbrace{x_{nd}}_{\substack{\text{obser-} \\ \text{vation}}}|\overbrace{\mathbf{u}_d}^{\text{codes}},\underbrace{\mathbf{z}_n}_{\substack{\text{latent}\\ \text{rprsnt.}}},\overbrace{\lambda}^{\substack{\text{disper-}\\ \text{sion}}})= \begin{cases} \big(1+\exp[-\lambda]\big)^{-1};\;&\text{if}\;\color{darkgreen}{x_{nd}=\min(1,\mathbf{z}_n^T\mathbf{u}_d)}\;\; \\ \big(1+\exp[\lambda]\big)^{-1};\;&\text{else} \end{cases}
$$
</p></span>
<span class="fragment appear"><p>
$$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \sigma_{\substack{\text{logistic} \\ \text{sigmoid}}}\left[\lambda \underbrace{\tilde{x}_{nd}}_{\tilde{x} = 2x-1} \left(1-2\color{brown}{\prod\limits_{l}(1-z_{nl}u_{ld})}\right) \right]$$
</p></span>
</div>
<aside class="notes">
<ul class="org-ul">
<li>This is a probabilistic model for <b>Boolean matrix factorisation</b>.
</li>
<li>Likelihood can be efficiently evaluated
</li>
<li>Example topic models
</li>
</ul>

</aside>
</section>
</section>
<section>
<section id="slide-sec-3">
<h2 id="sec-3">Inference for the OrMachine</h2>
<p>
<b>Gibbs Sampling</b>
</p>
<div class="column" style="float:left; width: 100%">
<div class="column" style="float:left; width: 60%">
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
Full conditional
</p>
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
$$ p(z_{nl}|\text{rest}) = \sigma\bigg[\lambda \tilde{z}_{nl} \sum\limits_d \tilde{x}_{nd}\; \color{darkgreen}{u_{ld}} \color{brown}{\prod\limits_{l'\neq l} (1-z_{nl'}u_{l'd})}\bigg] $$
</p>
</div>

<div class="column" style="float:left; width: 35%">
<span class="fragment (appear)" data-fragment-index="2"><p>
<p>
Computational shortcuts
</p>
<ul>
<li>\(\color{darkgreen}{u_{ld} = 0}\) <br> &rarr; \(z_{nl}\) and \(x_{nd}\) <i>disconnected</i>.</li>
<li>\(\color{brown}{z_{nl'}u_{l'd} = 1}\) for \(\color{brown}{l' \neq l}\) <br> &rarr; \(x_{nd}\) is <i>explained away</i>.</li>

</ul>
</div>

<span class="fragment (appear)" data-fragment-index="3"><p>
<p>
<b>Dispersion parameter set to maximise likelihood</b>
</p>
<span class="fragment (appear)" data-fragment-index="3"><p>
<p>
$$ \lambda_{\text{MLE}} = \text{logit}\left[ \text{reconstruction accuracy} \right] $$
</p>
</div>

<div class="column" style="float:left; width: 100%">
<span class="fragment (appear)" data-fragment-index="4"><p>
<p>
<b>Modified Sampler &#x2013; Always propose to change</b>
</p>
<div class="column" style="float:left; width: 50%">
<span class="fragment (appear)" data-fragment-index="4"><p>
<p>
Standard Gibbs sampler <br> 
  <img src="./heads_smaller.png" alt="heads_smaller.png" />   <img src="./heads_smaller.png" alt="heads_smaller.png" />  <img src="./heads_smaller.png" alt="heads_smaller.png" />  <img src="./tails_smaller.png" alt="tails_smaller.png" />  <img src="./heads_smaller.png" alt="heads_smaller.png" /> 
 <img src="./tails_smaller.png" alt="tails_smaller.png" />  <img src="./tails_smaller.png" alt="tails_smaller.png" />  <img src="./heads_smaller.png" alt="heads_smaller.png" />  <img src="./tails_smaller.png" alt="tails_smaller.png" /> 
</p>
</div>
<div class="column" style="float:left; width: 50%">
<span class="fragment (appear)" data-fragment-index="4"><p>
<p>
Metropolised Gibbs sampler <br> 
  <img src="./heads_smaller.png" alt="heads_smaller.png" />  <img src="./tails_smaller.png" alt="tails_smaller.png" />   <img src="./heads_smaller.png" alt="heads_smaller.png" />  <img src="./tails_smaller.png" alt="tails_smaller.png" />  <img src="./heads_smaller.png" alt="heads_smaller.png" />  <img src="./tails_smaller.png" alt="tails_smaller.png" />  <img src="./heads_smaller.png" alt="heads_smaller.png" />  <img src="./tails_smaller.png" alt="tails_smaller.png" />  <img src="./heads_smaller.png" alt="heads_smaller.png" />
</p>
</div>


<aside class="notes">
<ul class="org-ul">
<li>Use graphical model language -&gt; prior knowledge is clamping variables
</li>
<li>Conditional takes a surprisingly simple form
</li>
<li>You might think this is slow
</li>
<li>Think if this as a neural net without nonlinearity
</li>
<li>Normally need to consider <b>full Markov blanket</b>
</li>
<li>Parallelisable
</li>
<li>Set lambda to MLE after every sweep through u/z in <b>Monte Carlo EM fashion</b>
</li>
<li>Parallelisable
</li>
</ul>

</aside>

</section>
</section>
<section>
<section id="slide-sec-4">
<h2 id="sec-4">Examples and Experiments</h2>
<div class="outline-text-2" id="text-4">
</div></section>
<section id="slide-sec-4-1">
<h3 id="sec-4-1">Synthetic Data Benchmarks</h3>
<div class="column" style="float:left; width: 50%">
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
<b>Random Matrix Factorisation</b>
</p>

<div class="figure">
<p><img src="./factorsiation_performance_new2.png" alt="factorsiation_performance_new2.png" data-fragment-index="1" class="fragment appear" width="75%" height="%0%" />
</p>
</div>

<p>
[Message Passing: Ravanbakshs et al., ICML 2016]
</p>
</div>

<div class="column" style="float:left; width: 50%">
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
Problem Setting
</p>

<div class="figure">
<p><img src="./factor_example2.png" alt="factor_example2.png" data-fragment-index="1" class="fragment appear" width="95%" height="95%" />
</p>
</div>



<span class="fragment (appear)" data-fragment-index="2"><p>
<p>
<b>Random Matrix Completion</b>
</p>

<div class="figure">
<p><img src="./completion1.png" alt="completion1.png" data-fragment-index="2" class="fragment appear" width="60%" height="60%" />
</p>
</div>
</div>




<aside class="notes">
<ul class="org-ul">
<li>Compare to state of the art
</li>
<li>Movie lense results are in the paper
</li>
</ul>

</aside>
</section>
<section id="slide-sec-4-2">
<h3 id="sec-4-2">Single cell data II - 1.3 Million Brain Cells x 20k genes (E18 Mice)</h3>
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
<img src="./mouse_gene_single.png" alt="mouse_gene_single.png" width="110%" height="110%" />
Codes: Gene sets
</p>
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
<img src="./mouse_specimen_single.png" alt="mouse_specimen_single.png" width="110%" height="110%" />
Latent representations of each cell
$$ $$
</p>
<span class="fragment (appear)"><p>
<b>How to chose the latent dimension?</b>
</p></span>

<aside class="notes">
<ul class="org-ul">
<li>Convergence in a few hours on a Desktop with 8 cores. <b>Parallelsiable</b>!
</li>
</ul>

</aside>

</section>
<section id="slide-sec-4-2-1">
<h4 id="sec-4-2-1">Calculator Digit Hierarchy</h4>
<div class="column" style="float:left; width: 100%">

<div class="figure">
<p><img src="./calc_hierarchy_may5.png" alt="calc_hierarchy_may5.png" />
</p>
</div>
<ul>
<li>OrMachines of different dimensionality on noise-free calculator digits</li>

</ul>
</div>
</section>
<section id="slide-sec-4-2-2">
<h4 id="sec-4-2-2">Gene patterns &#x2013; Cell representations</h4>
<div class="column" style="float:left; width: 100%">

<div class="figure">
<p><img src="./mice_neurons1.png" alt="mice_neurons1.png" width="73%" height="50%" />
</p>
</div>

<div class="figure">
<p><img src="./mice_neurons2.png" alt="mice_neurons2.png" data-fragment-index="1" class="fragment appear" width="73%" height="50%" />
</p>
</div>
</div>

</section>
<section id="slide-sec-4-3">
<h3 id="sec-4-3">Mutations and Cellular Pathways in Different Cancer Types</h3>

</section>
<section id="slide-sec-4-4">
<h3 id="sec-4-4">Compose Layers of OrMachines</h3>
<div class="column" style="float:left; width: 19%">

<div class="figure">
<p><img src="./compose_mutations.png" alt="compose_mutations.png" width="100%" height="100%" /> 
</p>
</div>
</div>
<div class="column" style="float:left; width: 3%">
<p>
$$ $$
$$ $$
$$ $$
\(\mathbf{\otimes}\)
</p>
</div>
<div class="column" style="float:left; width: 19%">
<p>
<img src="./compose_pws.png" alt="compose_pws.png" width="100%" height="100%" />
$$ $$
$$ $$
</p>
<span class="fragment (appear)" data-fragment-index="3"><p>

<div class="figure">
<p><img src="./compose_disease.png" alt="compose_disease.png" width="100%" height="100%" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 5%">
<p>
$$ $$
$$ $$
$$ $$
</p>
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
\(\mathbf{\;\rightarrow\;}\)
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
</p>
<span class="fragment (appear)" data-fragment-index="3"><p>
<p>
\(\mathbf{\;\;\;\;\otimes}\)
</p>
</div>
<div class="column" style="float:left; width: 25%">
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
<img src="./compose_pw_patients_empty.png" alt="compose_pw_patients_empty.png" width="100%" height="100%" />
$$ $$
$$ $$
</p>
<span class="fragment (appear)" data-fragment-index="3"><p>

<div class="figure">
<p><img src="./compose_set_sets_empty.png" alt="compose_set_sets_empty.png" width="75%" height="75%" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 5%">
<p>
$$ $$
$$ $$
$$ $$
</p>
<span class="fragment (appear)" data-fragment-index="2"><p>
<p>
\(\mathbf{\;\leftarrow}\)
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
</p>
<span class="fragment (appear)" data-fragment-index="3"><p>
<p>
\(\mathbf{\rightarrow\;\;}\)
</p>
</div>

<div class="column" style="float:left; width: 23%">
<span class="fragment (appear)" data-fragment-index="2"><p>
<p>
<img src="./compose_pw_sets_empty.png" alt="compose_pw_sets_empty.png" width="85%" height="85%" />
<br>
</p>
<span class="fragment (appear)" data-fragment-index="2"><p>
<p>
\(\mathbf{\otimes}\)
</p>
<span class="fragment (appear)" data-fragment-index="2"><p>

<div class="figure">
<p><img src="./compose_embedding_empty.png" alt="compose_embedding_empty.png" width="85%" height="85%" /> 
</p>
</div>
</div>

<aside class="notes">
<ul class="org-ul">
<li><b>So this illustrates how this approach infers biologically meaningful differences and communalities between and within patient groups, that can lead to testable hypothesis, rather than optimising for classificatoin or prediction performance as continuous representations would.</b> 
</li>

<li>Discrete latent variables lead to testable hypothesis rather than classification or prediction performance 
</li>
<li>From a Bayesian perspective this is like a prior, indicating more factorisable structure.
</li>
<li>This expresses the prior belief that patients with the same disease share latent properties.
</li>
</ul>

</aside>

</section>
<section id="slide-sec-4-4-1" data-transition="none">
<h4 id="sec-4-4-1">Compose Layers of OrMachines</h4>
<div class="column" style="float:left; width: 19%">

<div class="figure">
<p><img src="./compose_mutations.png" alt="compose_mutations.png" width="100%" height="100%" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 3%">
<p>
$$ $$
$$ $$
$$ $$
\(\mathbf{\otimes}\)
</p>
</div>
<div class="column" style="float:left; width: 19%">
<p>
<img src="./compose_pws.png" alt="compose_pws.png" width="100%" height="100%" />
$$ $$
$$ $$
</p>

<div class="figure">
<p><img src="./compose_disease.png" alt="compose_disease.png" width="100%" height="100%" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 5%">
<p>
$$ $$
$$ $$
$$ $$
\(\mathbf{\;\rightarrow\;}\)
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
\(\mathbf{\;\;\;\;\otimes}\)
</p>
</div>
<div class="column" style="float:left; width: 25%">
<p>
<img src="./compose_pw_patients.png" alt="compose_pw_patients.png" width="100%" height="100%" />
$$ $$
$$ $$
</p>

<div class="figure">
<p><img src="./compose_set_sets.png" alt="compose_set_sets.png" width="75%" height="75%" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 5%">
<p>
$$ $$
$$ $$
$$ $$
\(\mathbf{\;\leftarrow}\)
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
$$ $$
\(\mathbf{\rightarrow\;\;}\)
</p>
</div>

<div class="column" style="float:left; width: 23%">
<p>
<img src="./compose_pw_sets.png" alt="compose_pw_sets.png" width="85%" height="85%" />
<br>
\(\mathbf{\otimes}\)
</p>

<div class="figure">
<p><img src="./compose_embedding.png" alt="compose_embedding.png" width="85%" height="85%" /> 
</p>
</div>
</div>

</section>
<section id="slide-sec-4-4-2">
<h4 id="sec-4-4-2">Embedding</h4>
<div class="column" style="float:left; width: 100%">

<div class="figure">
<p><img src="./pancan_representations.png" alt="pancan_representations.png" width="75%" height="75%" />
</p>
</div>
</div>
</section>
</section>
<section>
<section id="slide-sec-5">
<h2 id="sec-5">Conclusion</h2>
<div class="column" style="float:left; width: 100%">
<ul>
<li data-fragment-index="1" class="fragment appear">Outperforms available methods for Boolean Matrix Factorisation</li>
<li data-fragment-index="2" class="fragment appear">Applicable and scalable to most state-of-the art genomics data.</li>
<li data-fragment-index="3" class="fragment appear">Compose OrMachines to link different types of knowledge.</li>

</ul>
</div>
</section>
<section id="slide-sec-5-1">
<h3 id="sec-5-1">Acknowledgements</h3>
<p>
<b>Supervision</b>
</p>

<div class="column" style="float:left; width: 33%">
<p>
<img src="./chris.jpg" alt="chris.jpg" width="45%" height="45%" /> <br>
Chris Yau
</p>
</div>

<div class="column" style="float:left; width: 33%">
<p>
<img src="./michalis.png" alt="michalis.png" width="45%" height="45%" /> <br>
Michalis Titsias
</p>
</div>

<div class="column" style="float:left; width: 33%">
<p>
<img src="./ch.jpg" alt="ch.jpg" width="37%" height="37%" /> <br>
Chris Holmes
</p>
</div>

<div class="column" style="float:left; width: 100%">
<p>
<b>Funding</b>
</p>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./epsrc.png" alt="epsrc.png" width="40%" height="50%" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./roche.png" alt="roche.png" width="40%" height="50%" />
</p>
</div>
</div>
</section>
</section>
<section>
<section id="slide-sec-6">
<h2 id="sec-6">Additional Material</h2>
<div class="outline-text-2" id="text-6">
</div></section>
<section id="slide-sec-6-1">
<h3 id="sec-6-1">Deep noisy calculator digits</h3>
<div class="column" style="float:left; width: 100%">

<div class="figure">
<p><img src="./deeper_calc.png" alt="deeper_calc.png" width="50%" height="50%" />
</p>
</div>
<ul>
<li>Input: 50 digits with 70% missing observations</li>
<li>Reduce reconstruction error from 1.4% to 0.4% compared to shallow model</li>

</ul>
</div>
<aside class="notes">
<ul class="org-ul">
<li>From a Bayesian perspective this is like a prior, indicating more factorisable structure.
</li>
</ul>

</aside>
</section>
<section id="slide-sec-6-2">
<h3 id="sec-6-2">Auto-Regulating Sparsity</h3>
<div class="column" style="float:left; width: 100%">

<div class="figure">
<p><img src="./single_cell_overfit.png" alt="single_cell_overfit.png" width="70%" height="70%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-6-3">
<h3 id="sec-6-3">Preprint on ArXiv</h3>

<div class="figure">
<p><img src="./arxiv.png" alt="arxiv.png" width="90%" height="90%" />
</p>
</div>
</section>
<section id="slide-sec-6-4">
<h3 id="sec-6-4">Hamming Machine</h3>
<ul>
<li data-fragment-index="1" class="fragment appear">Construct a probability distribution based on the hamming distance between two binary vectors, \({h(\mathbf{x},\mathbf{u})}\), and a dispersion parameter \({\lambda}\): $$ p(\mathbf{x}|\mathbf{u}) \propto \exp\left[ -\lambda \, h(\mathbf{x},\mathbf{u}) \right] $$</li>
<li data-fragment-index="2" class="fragment appear">Each observations \({\mathbf{x} }\) is generated from a subset of binary <b>codes</b>: \({\mathbf{u}_{l{=}1\ldots L}}\), selected by a vector of binary latent variables \({\mathbf{z}}\) $$ p(\mathbf{x}|\mathbf{U},\mathbf{z},\lambda) \propto \prod\limits_l p(\mathbf{x}|\mathbf{u}_l,\lambda)^{z_l} = \prod\limits_d \exp\left[- \sum_l z_l \lambda h(x_d,u_{ld}) \right]$$</li>
<li data-fragment-index="3" class="fragment appear">Normalising the likelihood for for binary observations yields a <b>logistic sigmoid</b>: $$ p(x_d = 1|\mathbf{z}, \mathbf{u}_{1\ldots L}, \lambda) = \frac{1}{1+\exp\left[-\lambda \sum\limits_l z_l (2u_{ld} - 1) \right]} = \sigma\left[\lambda \sum_l z_l \tilde{u}_{ld} \right]$$</li>
<li data-fragment-index="4" class="fragment appear">We defined the mapping from \({\{0,1\}}\) to \({\{{-}1,1\}\,}\): \(\;\;{\tilde{u} = 2u{-}1}\)</li>

</ul>
<aside class="notes">
<ul class="org-ul">
<li>We use the tilde mapping throughout
</li>
<li>This migh be a bit unconventional
</li>
</ul>

</aside>
</section>
<section id="slide-sec-6-5">
<h3 id="sec-6-5">One-hot sampling</h3>
</section>
<section id="slide-sec-6-6">
<h3 id="sec-6-6">Introduction to Latent Variable Models</h3>
<aside class="notes">
<ul class="org-ul">
<li>I am sure you are all familiar with the notion of latent variable models. However, I'd like to use this section to introduce the notation and to set the stage for what follows.
</li>
<li>Latent variables are often thought of as underlying, unobserved properties that explain the observed data. For example states of disease that explain physiological parameters. Another example for a latent variable could be a persons particular taste that explains which product they buy or which movie they watch.
</li>
</ul>

</aside>
</section>
<section id="slide-sec-6-6-1">
<h4 id="sec-6-6-1">Notation and Graphical Model</h4>
<aside class="notes">
<ul class="org-ul">
<li>graphical model (directed = Bayes nets) encodes independence properties
</li>
<li>Everyone familiar with plate notation?
</li>
<li>from a frequentist point of view there is a difference between latent variables and paramters, for a Bayesian there isn't.
</li>
<li>continuous variables are often inappropriate
</li>
</ul>

</aside>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./plate_model.png" alt="plate_model.png" class="fragment (appear appear appear) :frag_idx(1)" />
</p>
</div>
<ul>
<li class="fragment appear">Mixture models</li>
<li class="fragment appear">Factor Analysis (PCA)</li>

</ul>
</div>
<div class="column" style="float:left; width: 50%">
<ul>
<li class="fragment appear">Variables
<ul>
<li>\({x_{nd}}\) &#x2013; observations</li>
<li>\({u_{ld}}\) &#x2013; parameters (globale variables, weights)</li>
<li>\({z_{nl}}\) &#x2013; latent variables (local variables)</li>

</ul></li>
<li class="fragment appear">Indices
<ul>
<li>\({n = 1\ldots N}\) &#x2013; observations/specimens</li>
<li>\({d = 1\ldots D}\) &#x2013; features (e.g. pixels or genes)</li>
<li>\({l = 1\ldots L}\) &#x2013; latent dimensions</li>
<li>\({k = 1\ldots K}\) &#x2013; layers</li>

</ul></li>
<li class="fragment appear">N observations</li>
<li class="fragment appear">D features</li>
<li class="fragment appear">L latent variables</li>
<li class="fragment appear">K layers / abstraction levels</li>

</ul>
</div>

</section>
<section id="slide-sec-6-6-2">
<h4 id="sec-6-6-2">Neural network</h4>

<div class="figure">
<p><img src="./single_layer_network.png" alt="single_layer_network.png" />
</p>
</div>
<ul>
<li class="fragment appear">Major difference to feed forward neural nets: Nodes <b>and</b> weights are stochastic</li>

</ul>
</section>
<section id="slide-sec-6-6-3">
<h4 id="sec-6-6-3">What makes a good latent variable model for biological data?</h4>
<aside class="notes">
<ul class="org-ul">
<li>Latent properties will often be discrete.
</li>
<li>Scale well
</li>
</ul>

</aside>
</section>
<section id="slide-sec-6-7">
<h3 id="sec-6-7">Multi-layer OrMachine</h3>

<div class="figure">
<p><img src="./twolayer_hm.png" alt="twolayer_hm.png" />
</p>
</div>

<p>
With \({\mathbf{z}^{[0]}_n = \mathbf{x}_n}\) and \({L^{[0]} = D}\), that is
$$  p(\mathbf{Z}^{[0:K]},\mathbf{U}^{[1:K]},\lambda) = 
  p(\mathbf{Z}^{[K]}) \prod_{k=0}^{K-1} p(\mathbf{Z}^{[k]}|\mathbf{Z}^{[k{+}1]},\mathbf{U}^{[k{+}1]},\lambda^{[k{+}1]})\, p(\mathbf{U}^{[k{+}1]})\, p(\lambda^{[k{+}1]}) 
$$
</p>

<p>
The joint density factorises in terms of the form p(layer|parents)
</p>
</section>
<section id="slide-sec-6-8">
<h3 id="sec-6-8">Random matrix factorisation</h3>
<div class="outline-text-3" id="text-6-8">
</div></section>
<section id="slide-sec-6-8-1">
<h4 id="sec-6-8-1">Problem setting</h4>

<div class="figure">
<p><img src="./factorisation.png" alt="factorisation.png" width="50%" height="50%" />
</p>
</div>
</section>
<section id="slide-sec-6-9">
<h3 id="sec-6-9">Speed</h3>

<div class="figure">
<p><img src="./scaling_parallel.png" alt="scaling_parallel.png" />
</p>
</div>
</section>
<section id="slide-sec-6-10">
<h3 id="sec-6-10">Single cell data I</h3>

<div class="figure">
<p><img src="./sc_hierarchy.png" alt="sc_hierarchy.png" />
</p>
</div>
</section>
<section id="slide-sec-6-11">
<h3 id="sec-6-11">MNIST</h3>

<div class="figure">
<p><img src="./mnist_hierarchy.png" alt="mnist_hierarchy.png" width="60%" height="50%" />
</p>
</div>
<aside class="notes">
<ul class="org-ul">
<li>Explain overfitting (get more and more distributed representation)
</li>
</ul>

</aside>
</section>
<section id="slide-sec-6-12">
<h3 id="sec-6-12">Deep calculator digits</h3>

<div class="figure">
<p><img src="./calc_digit_intro.png" alt="calc_digit_intro.png" />
</p>
</div>

<div class="figure">
<p><img src="./deep_calc.png" alt="deep_calc.png" width="50%" height="50%" />
</p>
</div>
<ul>
<li>Second layer representation fed forward to data layer.</li>

</ul>

</section>
<section id="slide-sec-6-13">
<h3 id="sec-6-13">A little detour: Peskun's Theorem</h3>
<ul>
<li data-fragment-index="1" class="fragment appear">We have
<ul>
<li data-fragment-index="1" class="fragment appear">A random variable \(X\) following a distribution \(\pi\)</li>
<li data-fragment-index="2" class="fragment appear">Transition matrices \(P_1\) and \(P_2\) that are reversible for \(\pi\): $$ \pi(x)P(x,y) = \pi(y)P(y,x) $$</li>
<li data-fragment-index="3" class="fragment appear">Define \(P_2 \ge P_1\), if it's true for every off-diagonal element.</li>

</ul></li>
<li data-fragment-index="4" class="fragment appear">The theorem states, if $$P_2 \ge P_1$$ then:
$$ v(f, \pi, P_1) \ge v(f, \pi, P_2) $$ where $$ v(f, \pi, P) = \lim_{N\rightarrow\infty} N \text{var}(\hat{I}_N) $$ is the variance of some estimator $$ \hat{I}_N = \sum\limits_{t=1}^N \frac{f(X^{(t)})}{N}\;\; \text{of}\;\; I = E_{\pi}(f)$$</li>

</ul>

</section>
<section id="slide-sec-6-14">
<h3 id="sec-6-14">Implementation</h3>
<div class="column" style="float:left; width: 100%">

<div class="figure">
<p><img src="./alg1_2.png" alt="alg1_2.png" width="50%" height="60%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-6-15">
<h3 id="sec-6-15">MovieLense</h3>
<div class="column" style="float:left; width: 50%">
<p>
<img src="./movielense1.png" alt="movielense1.png" width="85%" height="50%" />
Percentages of correctly predicted, unobserved movie ratings.
</p>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./ml_roc.png" alt="ml_roc.png" width="85%" height="50%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-6-16">
<h3 id="sec-6-16">Random matrix factorisation</h3>
<div class="column" style="float:left; width: 50%">
<p>
$$ $$
<img src="./mp.png" alt="mp.png" />
</p>
<ul>
<li>MAP inference using message passing.</li>
<li>Outperforms all previous state-of-the-art methods.</li>

</ul>
<ul data-fragment-index="1" class="fragment appear">
<li><b>OrMachine features consistently lower reconstruction error</b></li>

</ul>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./factorsiation_performance_new.png" alt="factorsiation_performance_new.png" data-fragment-index="1" class="fragment appear" width="70%" height="%0%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-6-17">
<h3 id="sec-6-17">Random matrix completion</h3>
<div class="column" style="float:left; width: 100%">
<ul>
<li data-fragment-index="1" class="fragment appear">Missing dat? Set unobserved data-point to \(x_{nd} = 0.5 \;\rightarrow\; \tilde{x}_{nd}=0\)</li>

</ul>
<span class="fragment (appear)" data-fragment-index="2"><p>
<p>
$$L = \prod\limits_{nd} \sigma\left[\lambda \tilde{x}_{nd} (1-2\prod\limits_{l}(1-z_{nl}u_{ld}) \right]\;\;\rightarrow\;\text{Contribute constant factor}\;\sigma(0)=\frac{1}{2}$$ 
</p>
<span class="fragment (appear)" data-fragment-index="3"><p>
<p>
$$ p(z_{nl}|\text{rest}) = \sigma\left[\lambda \tilde{z}_{nl} \sum\limits_d \tilde{x}_{nd}\; u_{ld}\prod\limits_{l'\neq l} (1-z_{nl'}u_{l'd})\right]\;\;\rightarrow\; \text{No contribution} $$
</p>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./completion1.png" alt="completion1.png" data-fragment-index="4" class="fragment appear" width="80%" height="50%" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./completion2.png" alt="completion2.png" data-fragment-index="5" class="fragment appear" width="85%" height="50%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-6-18">
<h3 id="sec-6-18">Dispersion paramter \(\lambda\)</h3>
<div class="column" style="float:left; width: 100%">
<p>
$$ $$
</p>
<ul>
<li>How many entries are correctly predicted by the deterministic Boolean product? $$ P = \sum\limits_{n,d} I\left[x_{nd}=(1-2\prod\limits_{l}(1-z_{nl}u_{ld}))\right] $$</li>
<li>We can rewrite the likelihood
$$ L = \sigma(\lambda)^P \sigma(-\lambda)^{(ND-P)} $$</li>
<li>We find the MLE of \(\sigma(\lambda)\) in <b>closed form</b>:</li>

</ul>
<p>
$$ \sigma(\lambda)_{\text{mle}} =\frac{P}{ND}\;. $$
</p>
</div>
</section>
<section id="slide-sec-6-19">
<h3 id="sec-6-19">Metropolised Gibbs sampler - Algorithm</h3>
<div class="column" style="float:left; width: 100%">

<div class="figure">
<p><img src="./alg2_mod.png" alt="alg2_mod.png" width="50%" height="60%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-6-20">
<h3 id="sec-6-20">"Data"</h3>
<div class="column" style="float:left; width: 100%">

<div class="figure">
<p><img src="./pancan_data.png" alt="pancan_data.png" width="80%" height="80%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-6-21">
<h3 id="sec-6-21">Clustering via one-hot activations</h3>
<div class="column" style="float:left; width: 100%">

<div class="figure">
<p><img src="./pancan_clustering.png" alt="pancan_clustering.png" width="60%" height="60%" />
</p>
</div>
</div>

</section>
<section id="slide-sec-6-22">
<h3 id="sec-6-22">A modified binary state Gibbs sampler</h3>
<div class="column" style="float:left; width: 100%">
<ul>
<li>Gibbs sampler: Draw a new value \(z'\) from the full conditional \(p(z'|\text{rest})\).</li>
<li>Here, we propose value \(z'\) <b>different from the</b> previous value \(z\) with probability 1.</li>
<li>Metropolis-Hasting: $$ p(\text{accept}) = p(\text{mutate})= \frac{p(z'|\text{rest}) q(z|z')}{p(z|\text{rest}) q(z'|z)} = \frac{p(z'|\text{rest})}{1-p(z'|\text{rest})} \ge p(z'|\text{rest})$$</li>

</ul>

<ul class="fragment appear">
<li>Typical Gibbs sampler:

<p>
<img src="./heads_small.png" alt="heads_small.png" />   <img src="./heads_small.png" alt="heads_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" /> 
 <img src="./tails_small.png" alt="tails_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./heads_small.png" alt="heads_small.png" /> <img src="./tails_small.png" alt="tails_small.png" /> 
</p></li>

</ul>

<ul class="fragment appear">
<li>Metropolised Gibbs sampler:

<p>
<img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />   <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />  <img src="./heads_small.png" alt="heads_small.png" />  <img src="./tails_small.png" alt="tails_small.png" />
</p></li>

</ul>
</div>
<aside class="notes">
<ul class="org-ul">
<li>Lambda is available in closed form
</li>
<li>Monte Carlo EM fashion
</li>
</ul>

</aside>

</section>
<section id="slide-sec-6-23">
<h3 id="sec-6-23">Inference: Monte-Carlo EM Algorithm</h3>
<div class="column" style="float:left; width: 100%">
<div class="column" style="float:centre; align=centre">
<ul>
<li><code>Until stopping criterion is reached</code>
  $$ $$
<ul>
<li><code>For each factor matrix entry</code> \(u_{ld}, z_{nl}\) <code>[in parallel]</code>
<ul>
<li><code>Compute full conditional (using shortcuts)</code></li>
<li><code>Update entry following Metropolised Gibbs sampler</code></li>

</ul></li>

</ul>
<p>
$$ $$
</p>
<ul>
<li><code>Set</code> \(\sigma(\lambda)\) <code>to its MLE</code> 
   \(\big[\sigma(\lambda)_{\text{mle}}=\) <code>MAP reconstruction accuracy</code> \(\big]\)</li>

</ul></li>

</ul>
</div>
</div>

</section>
<section id="slide-sec-6-24">
<h3 id="sec-6-24">Full conditionals</h3>
<div class="column" style="float:left; width: 60%">
<p>
$$ p(z_{nl}|\text{rest}) = \sigma\bigg[\lambda \tilde{z}_{nl} \sum\limits_d \tilde{x}_{nd}\; \color{darkgreen}{u_{ld}} \color{brown}{\prod\limits_{l'\neq l} (1-z_{nl'}u_{l'd})}\bigg] $$
</p>
<ul data-fragment-index="3" class="fragment appear">
<li>Intuition: Need to consider the full Markov Blanket.</li>

</ul>
<p>
$$ $$
</p>
<ul>
<li data-fragment-index="4" class="fragment appear"><b>Computational shortcut:</b>
<ul>
<li data-fragment-index="5" class="fragment appear">\(\color{darkgreen}{u_{ld} = 0}\) &rarr; No effect of \(z_{nl}\) on the likelihood.</li>
<li data-fragment-index="6" class="fragment appear">\(\color{brown}{z_{nl'}u_{l'd} = 1}\) for \(\color{brown}{l' \neq l}\) \(\rightarrow\) \(x_{nd}\) is <b>explained away</b>.</li>

</ul></li>

</ul>
</div>
<div class="column" style="float:left; width: 40%">

<div class="figure">
<p><img src="./single_layer_network.png" alt="single_layer_network.png" data-fragment-index="3" class="fragment appear" />
</p>
</div>
</div>
<aside class="notes">
<ul class="org-ul">
<li>Prior knowledege by setting x=0.5
</li>
</ul>

</aside>
</section>
<section id="slide-sec-6-25">
<h3 id="sec-6-25">The Data Revolution in Biology</h3>
<div class="column" style="float:left; width: 100%">
<p>
Rapid increase in the availability of <b>large molecular datasets</b>!
</p>
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
$$\color{red}{\Large\mathbf{\downarrow}}$$
</p>
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
Better understanding of disease and <b>better healthcare</b>?
$$ $$
</p>
<span class="fragment (appear)" data-fragment-index="2"><p>
<p>
Need computational and statistical tools that
</p>
<span class="fragment (appear)" data-fragment-index="2"><p>
<ol>
<li><b>Scale</b> to the huge datastes</li>
<li>Relate to the <b>physical and biological mechanisms</b> that generate the data</li>
<li>Can leverage on prior <b>expert domain knowledge</b></li>
<li>Are easy to <b>interprete</b></li>

</ol>
</div>
<aside class="notes">
<p>
A data revolution has been going on for a while in Biology. Based on a rapidly declining cost of Genome sequencing techniques; on new experimantel techniques such as single cell sequencing; and on recent population scale studies we have tremendous amounts of molecular data for biomedical research.
Such data promises the precise understanding of the molecular basis of diseases, development targeted therapies and ultimately better healthcare
</p>

<p>
But to live up to this promise, we need tools that enable reseaercher to draw meaningful conclusioncs from such datasets. In particular we need statistical and computational models, that fullfill the following criteria. They need to scale to the enormous amounts of data, that can relate to the underlying physical mechanisms, they are able to leverage on the prior expert domain knowledge, and that are easily interpretable.
</p>

<p>
In order for latent variables to be interpretable in this setting, they will often need to be categorical or binary. For instance indicating the presence or absence of disease, or disruption of a cellular pathway process.
Latent variable models play an 
That's why I would like to introduce you to the OrMachine. 
The OrMachine is fully binary latent variable model, that is based on Boolen Matrix factorisatoin.
</p>

</aside>
</section>
<section id="slide-sec-6-26">
<h3 id="sec-6-26">Probabilistic Generative Model</h3>
<div class="column" style="float:left; width: 45%">

<div class="figure">
<p><img src="./calc_digit_intro.png" alt="calc_digit_intro.png" width="90%" height="90%" />
</p>
</div>
</div>

<div class="column" style="float:right; width: 55%">
<p>
Notation
</p>
<ul>
<li>\({x_{nd}}\) &#x2013; observations</li>
<li>\({u_{ld}}\) &#x2013; factor matrix: global codes</li>
<li>\({z_{nl}}\) &#x2013; factor matrix: local latent variables</li>
<li>\(\lambda \ge 0\) &#x2013; global noise parameter</li>

</ul>

<p>
Definitions
</p>
<ul>
<li>Mapping \(\{0,1\}\) to \(\{-1,1\}\): \(\tilde{x} = 2x-1\)</li>
<li>Logistic sigmoid: \(\sigma(x) = (1+\exp[-x])^{-1}\)</li>

</ul>
</div>

<div class="column" style="float:right; width: 100%">
<span class="fragment appear"><p>
$$  p(x_{nd}|\mathbf{u}_d,\mathbf{z}_n,\lambda)= \begin{cases} \sigma [ \lambda];\;&\text{if}\;\color{darkgreen}{x_{nd}=\min(1,\mathbf{z}_n^T\mathbf{u}_d)}\;\; \\ 1-\sigma [ \lambda]=\sigma[-\lambda];\;&\text{if}\;x_{nd}\neq\min(1,\mathbf{z}_n^T\mathbf{u}_d)    \end{cases}
$$
</p></span>
<span class="fragment appear"><p>
$$\;\;\;\; = \sigma\left[\lambda \tilde{x}_{nd} \left(1-2\color{brown}{\prod\limits_{l}(1-z_{nl}u_{ld})}\right) \right]$$
</p></span>
</div>
<aside class="notes">
<ul class="org-ul">
<li>This is a probabilistic model for <b>Boolean matrix factorisation</b>.
</li>
<li>Likelihood can be efficiently evaluated
</li>
<li>Example topic models
</li>
</ul>

</aside>
</section>
<section id="slide-sec-6-27">
<h3 id="sec-6-27">Unsupervised learning</h3>
<div class="column" style="float:left; width: 100%">
<span class="fragment (appear)" data-fragment-index="1"><p>
<p>
<div align="left"> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp; Key requirements </div>
</p>
<ul>
<li data-fragment-index="1" class="fragment appear">&#x2026; <b>interpretable</b></li>
<li data-fragment-index="2" class="fragment appear">&#x2026; relate to the physical <b>data-generating mechanism</b>.</li>
<li data-fragment-index="3" class="fragment appear">&#x2026; ability to utilise <b>prior expert knowledge</b>.</li>
<li data-fragment-index="4" class="fragment appear">&#x2026; <b>scalable</b></li>

</ul>
</div>

<aside class="notes">
<ul class="org-ul">
<li>Joint work with CY, MT and CH.
</li>
<li>Started out my PhD wanting to develop model for unsupervised learning
</li>
<li>The goal is to undertstand processes that underly high dimensional heterogeneous datasets
</li>
<li>Focus on Genomics, where the abundance of molecular data needs to be translated into understanding of disease and better healthcare
</li>
<li>Key requirements
</li>
</ul>

<p>
When I started my PhD research about 1 1/2 years ago, I was interested in unsupervised learning. I wanted to develope latent variable models and inference to work with large, complicated, noisy data sets. In particular, models that can help to build an intuition about the data generating processes and that enable us to derive scientific insights from the data. This is said mostly with Genomics data in mind, but applies in general.
</p>

<p>
Over the course of this work I identified four requirements that such a needs to meet.
</p>
<ul class="org-ul">
<li>interpretability: intuitive understaing for latent variables, need to quantify uncertainty
</li>
<li>physical mechanism: depends on the application. observations and meaningful latent properties are usually digital, e.g. presence/absence of disease or mutation; customer buying or not buying a product, the colour in a region of an image etc.
</li>
<li>prior knowledge: especially in biology we already know a lot, not everything needs to be inferred from the data.
</li>
<li>scalability
</li>
</ul>

<p>
So we developed a probabiistic model for Boolean matrix factorisation, that we call the OrMachine
</p>

</aside>









</section>
<section id="slide-sec-6-28">
<h3 id="sec-6-28">Setup: Combine Layers of OrMachines</h3>
<div class="column" style="float:left; width: 100%">

<div class="figure">
<p><img src="./arc.png" alt="arc.png" />
</p>
</div>
</div>
</section>
</section>
</div>
</div>

<script src="./reveal.js/lib/js/head.min.js"></script>
<script src="./reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: true,
rollingLinks: true,
keyboard: true,
overview: true,
width: 1920,
height: 1080,
margin: 0.15,
minScale: 0.50,
maxScale: 2.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'cube', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: './reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: './reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: './reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: './reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } }
]
});
</script>
</body>
</html>
